{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max\n",
    "from utils.constants import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CSV Worker\").config(\"spark.executor.memory\", \"2g\").config(\"spark.driver.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_311_df(df, keyword, column=None):\n",
    "    if column:\n",
    "        df = df.filter(col(column).like(f\"%{keyword}%\"))\n",
    "    else:\n",
    "        df = df.filter(\n",
    "            col(\"Descriptor\").like(f\"%{keyword}%\")\n",
    "            | col(\"Complaint Type\").like(f\"%{keyword}%\")\n",
    "        )\n",
    "    return df.drop(\"Descriptor\", \"Complaint Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_311_df = spark.read.parquet(str(Paths.RAW_DATA_PARQUET / \"user/base_311.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to aggregate the 311 dataset with the various government action datasets, we need to split it to so that the subsets can be matched. We need the following subsets:\n",
    "1. 311 pothole complaints (aggregated with pothole work orders dataset)\n",
    "2. 311 parking complaints (aggregated with parking violations dataset)\n",
    "3. 311 vacant lot complaitns (aggregated with vacant lot cleanings dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pothole_311_df = split_311_df(base_311_df, \"Pothole\",\"Descriptor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/03 19:36:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "parking_311_df = split_311_df(base_311_df, \"Parking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacant_311_df = split_311_df(base_311_df, \"Vacant Lot\", \"Complaint Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just for brevity, in reality, you'll need to free at least base_311_df, if not the other 2, before writing because this is too much to fit into memory all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"pothole\",\"parking\", \"vacant\"]\n",
    "dfs = [pothole_311_df,parking_311_df,vacant_311_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in zip(dfs, names):\n",
    "    df.write.parquet(str(Paths.RAW_DATA_PARQUET / f\"user/{name}_311.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simcity_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
